{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c489d89c-c7c0-4926-bc53-f27e995d8cec",
   "metadata": {
    "id": "c489d89c-c7c0-4926-bc53-f27e995d8cec"
   },
   "source": [
    "## $\\textcolor{green}{\\text{Information and Entropy}}$\n",
    "\n",
    "Entropy is a concept from information theory designed to describe amount of uncertainty or disorder we have in the given probability distribution. The idea is the more uncertain we are, the less information we have about what we can get.  To find entropy we need to calculate average amount of information we need to describe a random variable. In other words, the more information we need to k find out the value of a random variable, the more disorder we have, and so the more entropy we have. \n",
    "\n",
    "So, how do we measure information? We use \"bits\". A bit is a binary variable (0 or 1). So, we ask Yes/No questions and see how many questions on average we can ask to find out the value of a random variable. To see how this works, let's look at two examples.\n",
    "\n",
    "#### $\\textcolor{red}{\\text{Example 1}}$\n",
    "Supppose we have the following distribution:\n",
    "\n",
    "$$Q(x) = \\begin{cases}\n",
    "          0.25 & x =A \\\\\n",
    "          0.25 & x =B \\\\\n",
    "          0.25 & x =C \\\\\n",
    "          0.25 & x =D \\\\\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "Given a random variable $x$, how many questions can we ask to determine what it equals to?\n",
    "Since all probabilities are the same, the most efficient way is to ask two questions:\n",
    "\n",
    "1. Is it A or B?\n",
    "\n",
    "2. If the answer was \"Yes\", then we ask: Is it A?; If the answer was \"No\", we ask: Is it C?\n",
    "\n",
    "So, we always ask two questions, therefore the average number of questions asked is two as well. And so, we say that the entropy here is 2 bits. We can denote this by $H(Q)=2$.\n",
    "\n",
    "#### $\\textcolor{red}{\\text{Example 2}}$\n",
    "\n",
    "Suppose now we have the following non-uniform distribution:\n",
    "\n",
    "$$P(x) = \\begin{cases}\n",
    "          0.5 & x =A \\\\\n",
    "          0.25 & x =B \\\\\n",
    "          0.125 & x =C \\\\\n",
    "          0.125 & x =D \\\\\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "We can still determine the value of $x$ by asking the same two questions as in the example 1, but can we do better than that? Well, since the probability $P(x=A)$ is already a half, $A$ will appear very often, so let's ask our questions in the following way:\n",
    "\n",
    "1. Is it A?\n",
    "\n",
    "2. If the answer was \"No\", we ask: Is it B?\n",
    "\n",
    "3. If the answer was \"No\", we ask: Is it C?\n",
    "\n",
    "On the first glance, it may seem like we are asking more questions now, but it depends. If $x=A$ for example, we are asking only 1 question. So, what is the average number of questions we are asking? To find this, we just need to calculate the Expected Value of $x$. Let $N(x)$ denote number of questions we are asking to get to $x$ (so $N(A)=1$ and $N(C)=3$ for example). Then to find the expected value of $x$ we need to multiply probabilities of specific outcome $P(x)$ by how many questions we need to ask to get that outcome $N(x)$, and then add all the products:\n",
    "\n",
    "$$E(x)=\\sum_{x=A, B, C, D } P(x) \\cdot N(x)=0.5 \\cdot 1+0.25\\cdot 2+0.125\\cdot 3+0.125\\cdot 3=1.75$$\n",
    "\n",
    "So, on average we are asking less than two questions and we say that the entropy for this distribution is 1.75 bits, denoted by $H(P)=1.75$.\n",
    "\n",
    "In the example 2, we have less entropy, because we have more predictive power than in the example 1.\n",
    "\n",
    "#### $\\textcolor{blue}{\\text{Formalizing the formula}}$\n",
    "Let's formalize our calculation. Suppose we have some probability distribution $P(x)$ in which $P(i)=p_i$. Notice that from Example 2 that $N(x)$ depends on $P(x)$. The lower the probability, the more questions we need to ask, and since each question has a binary answer, we look at the powers of 2:\n",
    "\n",
    "$$N(x)=\\log_2\\left(\\frac{1}{P(x)}\\right)$$\n",
    "\n",
    "Then the entropy is:\n",
    "\n",
    "$$H(P)=\\sum_{i}p_i\\cdot N(i)=\\sum_{i}p_i\\cdot\\log_2(1/p_i) = - \\sum_{i}p_i\\cdot\\log_2(p_i)$$\n",
    "\n",
    "Finally, we will only care about entropy as a relative quantity and so we can replace $\\log_2()$ with $\\ln()$ as they differ only by a constant:\n",
    "\n",
    "$$\\textcolor{blue}{H(P)= - \\sum_{i}p_i\\cdot\\ln(p_i)}$$\n",
    "\n",
    "Note, we can use either natural logarithm and base2 logarithm. While they do produce different values, they keep the relative difference intact. This means if you want to compare entropies of diffeent distribution, you should use the same logarithm for both. Base 2 logarithm has real meaning (average number of question we have to ask), but natural logarithm is generally considered easier one to use.\n",
    "\n",
    "#### $\\textcolor{red}{\\text{Example 3}}$\n",
    "\n",
    "If we have a bag with 10 blue and 10 red marbles and we pick one marble at random, then we can't really predict what kind of marble we will get, since it could be either color with equal probability. However, if the bag had 19 red marbles and 1 blue marble, then we should be expecting to get a red marble. So, in the first case we have larger entropy.\n",
    "\n",
    "In our first bag case, entropy is\n",
    "\n",
    "$$H= -\\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\approx 0.6931$$\n",
    "\n",
    "And in the second case:\n",
    "\n",
    "$$H=-\\frac{1}{20}\\ln\\left(\\frac{1}{20}\\right)-\\frac{19}{20}\\ln\\left(\\frac{19}{20}\\right) \\approx 0.198514$$\n",
    "\n",
    "\n",
    "$\\bf{Note}$: If $p_k=0$, we would get $0 \\cdot \\ln(0)$ which is undefined. However, since 0 probability is a perfect certainty, we set this equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fBXQnsJ2I2E",
   "metadata": {
    "id": "1fBXQnsJ2I2E"
   },
   "source": [
    "## $\\textcolor{green}{\\text{Cross-Entropy and KL-Divergence}}$\n",
    "\n",
    "\n",
    "Suppose we have two distributions from examples 1 and 2. We will think of them as some kind of 4-sided dice. We would like to compare these two distributions. To do that, we look at the probabilities of getting specific samples. In other words, suppose I roll die $P$ 16 times. Then we would expect to get on average 8 $A$'s, 4 $B$'s , 2 $C$'s and 2 $D$'s. (In fact, if we roll the die $N$ times, then the expected amount of let's say $A$'s is $N \\cdot P(A) = N \\cdot 0.5$.)\n",
    "\n",
    "Suppose we get these values in this specific order (8 $A$'s followed by 4 $B$'s followed by 2 $C$'s followed by 2 $D$'s). Then the probability of getting this sequence is\n",
    "\n",
    "$$Prob=0.5^8 \\cdot 0.25^4 \\cdot 0.125^2 \\cdot 0.125^2$$\n",
    "\n",
    "This value is very tiny, and as $N$ grows, it gets even smaller. To fix this we will take $N$-th root to normalize it with respect to the sample size. Since the powers in the above calculations are of the form $N\\cdot p_i$, this root will eliminate $N$'s from the power, leaving just the probability.\n",
    "\n",
    "The second problem is that our numbers are still quite small and we are multiplying them which makes it even worse. Usual way to fix this is to take a logarithm. We will take a negative logarithm to keep our answer positive:\n",
    "\n",
    "$$-\\ln\\left(Prob^{1/N}\\right)=-\\frac{8}{16}\\ln(0.5)-\\frac{4}{16}\\ln(0.25)-\\frac{2}{16}\\ln(0.125)-\\frac{2}{16}\\ln(0.125)$$\n",
    "\n",
    "$$=-0.5\\ln(0.5)-0.25\\ln(0.25)-0.125\\ln(0.125)-0.125\\ln(0.125) \\approx 1.213$$\n",
    "\n",
    "Notice what we got is just entropy of $P$: $H(P)=-\\sum_i p_i \\ln(p_i)$. However, in this case we will think of this as cross-entropy of $P$ with respect to itself (I am using specific distribution ($P$) to get expected distribution in my sample and I am using specific distribution ($P$ again) to calculate the probability of getting that sample. So, I am using $P$ for both parts.)\n",
    "\n",
    "Now, what if we look for probability of getting the same sample (AAAAAAAA BBBB CC DD) but using die $Q$? The closer the $Q$ is to $P$ the closer the probabilites should be to the ones we got with a die $P$. Recall that probability of getting any of the values in $Q$ are all 0.25. So, we calculate:\n",
    "\n",
    "$$Prob=0.25^8 \\cdot 0.25^4 \\cdot 0.25^2 \\cdot 0.25^2$$\n",
    "\n",
    "Performing the same normalization:\n",
    "\n",
    "$$-\\ln\\left(Prob^{1/N}\\right)=-0.5\\ln(0.25)-0.25\\ln(0.25)-0.125\\ln(0.25)-0.125\\ln(0.25) \\approx 1.386$$\n",
    "\n",
    "What we got is a cross-entropy of $P$ relative to $Q$. To generalize this, let $p_i$'s be probabilties from $P$ and $q_i$'s be probabilities from $Q$, then the cross-entropy of $P$ relative to $Q$ is equal to\n",
    "\n",
    "$$\\textcolor{blue}{H(P,Q)=-\\sum_i p_i \\ln(q_i)}$$\n",
    "\n",
    "Few properties of cross-entropy:\n",
    "\n",
    "$\\textcolor{blue}{\\text{Properties of H}}$\n",
    "\n",
    "If $P$ and $Q$ are two probability distributions, then\n",
    "1. $H(P,P)=H(P)$\n",
    "\n",
    "2. $H(P,Q) \\geq H(P) \\geq 0$\n",
    "\n",
    "3. In general, $H(P,Q) \\neq H(Q,P)$\n",
    "\n",
    "\n",
    "Using these cross-entropies we can measure how far $Q$ is from $P$ by subtraction:\n",
    "\n",
    "$$\\textcolor{blue}{D_{KL}(P||Q)= H(P,Q)-H(P)}$$\n",
    "\n",
    "This is called $\\bf{\\text{KL-Divergence}}$. So, in our example,\n",
    "\n",
    "$$D_{KL}(P||Q) \\approx 1.386- 1.213=0.173$$\n",
    "\n",
    "$\\textcolor{blue}{\\text{Properties of KL-Divergence}}$:\n",
    "\n",
    "If $P$ and $Q$ are two probability distributions, then\n",
    "\n",
    "1. $D_{KL}(P||Q) \\geq 0$\n",
    "\n",
    "2. $D_{KL}(P||P) = 0$\n",
    "\n",
    "3. In general, $D_{KL}(P||Q) \\neq D_{KL}(Q||P) $\n",
    "\n",
    "The main difference between cross-entropy and KL-divergence is as follows:\n",
    "\n",
    "1. Cross-entropy $H(P,Q)$ is average amount of \"bits\" we need to represent event from $Q$ instead of $P$.\n",
    "2. KL-divergence $D_{KL}(P||Q)$ is average amount of extra \"bits\" we need to represent event from $Q$ instead of $P$.\n",
    "\n",
    "Since $D_{KL}(P||Q) \\neq D_{KL}(Q||P) $, KL-divergence is techically not a distance between two distributions.\n",
    "\n",
    "#### $\\textcolor{red}{\\text{Example 4}}$\n",
    "\n",
    "Suppose we have a third die $R$ with the following likelihoods:\n",
    "\n",
    "$$R(x) = \\begin{cases}\n",
    "          0.1 & x =A \\\\\n",
    "          0.1 & x =B \\\\\n",
    "          0.4 & x =C \\\\\n",
    "          0.4 & x =D \\\\\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "If you look carefully, it should be quite obvious that $R$ is much further from $P$ than $Q$ was. So let's see that by calculating $D_{KL}(P||R)$\n",
    "\n",
    "$$H(P, R)=-0.5\\ln(0.1)-0.25\\ln(0.1)-0.125\\ln(0.4)-0.125\\ln(0.4) \\approx 1.956$$\n",
    "\n",
    "And so, $$D_{KL}(P||R) \\approx 1.956- 1.213=0.743$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b3b12",
   "metadata": {
    "id": "c11b3b12"
   },
   "source": [
    "## $\\textcolor{green}{\\text{Cross-Entropy Loss}}$\n",
    "\n",
    "Suppose we have some labeled training data and we have some classification model parametrized with some parameter $\\theta$. Then, given a data point $x$, our model produces a prediction $\\hat{y}$. This prediction is a vector with number of entries equal to number of classes. Each entry is the probability that our data point belongs to a corresponding class. Such vector is called probability vector. For example, if\n",
    "$$\\hat{y}=\\begin{bmatrix} 0.2 \\\\ 0.3 \\\\0.5 \\end{bmatrix}$$\n",
    "Then the probability that $x$ belongs to class 0 is 0.2, the probability that $x$ belongs to class 1 is 0.3, and the probability that $x$ belongs to class 2 is 0.5. So, we get a distribution for $x$. I will denote this as $$ \\hat{P}(x|\\theta)=\\begin{bmatrix} \\hat{p}_1 \\\\ \\hat{p}_2 \\\\\\hat{p}_3 \\end{bmatrix}$$\n",
    "\n",
    "However, our data is labeled, so we do have the actual label $y$ for $x$. This is also a probability vector. However, it contains a single 1 in the position corresponding to a true class of $x$, and it has zeros everywhere else. Nevertheless, it is still a distribution, and we will denote it as\n",
    "\n",
    "$$P(x)=\\begin{bmatrix} {p}_1 \\\\ {p}_2 \\\\{p}_3 \\end{bmatrix}$$ Note that it doesn't depend on $\\theta$ since it doesn't come from our training model.\n",
    "\n",
    "We would like our model to predict the correct class, so we want $ \\hat{P}(x|\\theta)$ to be close to $P(x)$. We can use KL-divergence to see how close two distributions are and try to minimize it.\n",
    "\n",
    "$$D_{KL}\\left(P(x) || \\hat{P}(x|\\theta)\\right)=H\\left(P(x), \\hat{P}(x|\\theta)\\right) - H\\left(P(x)\\right)$$\n",
    "\n",
    "Two things to note here:\n",
    "\n",
    "1. We can not reverse distributions inside $D_{KL}$, since then we would have to use entropy of $\\hat{P}$. However, we don't want this entropy to affect our optimization.\n",
    "\n",
    "2. Since $H(P(x))$ doesn't depend on $\\theta$, our optimization doesn't depend on this term. In other words, the parameter $\\theta$ that minimizes KL-divergence will also minimize cross-entropy:\n",
    "\n",
    "$$\\mathop{\\arg\\min}_{\\theta} \\ D_{KL}\\left(P(x) || \\hat{P}(x|\\theta)\\right) = \\mathop{\\arg\\min}_{\\theta} \\ H\\left(P(x), \\hat{P}(x|\\theta)\\right)$$\n",
    "\n",
    "The $\\mathop{\\arg\\min}_{\\theta}$ means the value $\\theta$ that minimizes given quantity.\n",
    "\n",
    "So, we now define the Cross-Entropy Loss function for a single datapoint $x$ as follows :\n",
    "\n",
    "$$\\textcolor{blue}{CELoss(x)=H\\left(P(x), \\hat{P}(x|\\theta)\\right)=-\\sum_i p_i\\ln(\\hat{p}_i)}$$\n",
    "\n",
    "And for the whole dataset, as before, we find the average of losses over the whole dataset:\n",
    "\n",
    "$$\\textcolor{blue}{CELoss(X)=-\\frac{1}{N}\\sum_{x \\in X}CELoss(x)=-\\frac{1}{N}\\sum_{x \\in X}\\sum_i p_i\\ln(\\hat{p}_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RshaMBH2HOqx",
   "metadata": {
    "id": "RshaMBH2HOqx"
   },
   "source": [
    "## $\\textcolor{green}{\\text{Relation of CELoss to NLLoss}}$\n",
    "\n",
    "When we have only two classes, then\n",
    "\n",
    "$$P(x)=\\begin{bmatrix} p \\\\ 1-p  \\end{bmatrix}, \\hat{P}(x)=\\begin{bmatrix} \\hat{p} \\\\ 1-\\hat{p}  \\end{bmatrix}$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$CELoss(x)=-p\\ln(\\hat{p})-(1-p)\\ln(1-\\hat{p})=NLLoss(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0cb4b-b5d7-47da-b817-de3b2e4b8c44",
   "metadata": {},
   "source": [
    "##  $\\textcolor{green}{\\text{Homework}}$\n",
    "\n",
    "### Problem Set 1\n",
    "\n",
    "In Problem Set 1, use logarithm base 2 for calculating entropies.\n",
    "\n",
    "1. Suppose we have the following distribution $P$:\n",
    "    $$P(x) = \\begin{cases}\n",
    "          0.5 & x =A \\\\\n",
    "          0.3 & x =B \\\\\n",
    "          0.1 & x =C \\\\\n",
    "          0.1 & x =D \\\\\n",
    "       \\end{cases}$$\n",
    "\n",
    "    a. Compute Entropy for this distribution.\n",
    "\n",
    "    b. Suppose we get a random letter and we ask \"Is it A?\". If the answer is \"Yes\", then we are left with only one possibility: it is an A. So in this case, we have a new distribution that just says $P(A)=1$. What is the entropy of this distribution?\n",
    "\n",
    "    c. Suppose the answer to the \"Is it A?\" was a \"No\". What is the new distribution? and what is the entropy of it?  \n",
    "\n",
    "    d. Since the probability of \"Yes\" or \"No\" was 50% each in this case, we can find expected entropy of a new distrubution by just computing the average of the two new entropies. Find this expected entropy. Then subtract it from original entropy. \n",
    "\n",
    "    e. What do you think the final answer in part d. represents?\n",
    "\n",
    "\n",
    "\n",
    "2. Repeat problem 1, but in part b. ask \"Is is B or C?\" instead. Note that in part d. to find expected entropy you would need use the fact that the probability of \"Yes\" is 0.4 and probability of \"No\" is 0.6.\n",
    "\n",
    "3. In the above problems, which question was better \"Is it A?\" or \"Is it B or C?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83703a-2052-48db-be5f-035fe8d6bb3a",
   "metadata": {},
   "source": [
    "### Problem set 2\n",
    "\n",
    "Consider two distributions $Q$ and $R$:\n",
    "\n",
    "$$Q(x) = \\begin{cases}\n",
    "      0.55 & x =A \\\\\n",
    "      0.25 & x =B \\\\\n",
    "      0.05 & x =C \\\\\n",
    "      0.15 & x =D \\\\\n",
    "   \\end{cases}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$R(x) = \\begin{cases}\n",
    "      0.4 & x =A \\\\\n",
    "      0.3 & x =B \\\\\n",
    "      0.15 & x =C \\\\\n",
    "      0.15 & x =D \\\\\n",
    "   \\end{cases}$$\n",
    "\n",
    "1. Which of these two distributions is \"closer\" to the distribution $P$ from Problem Set 1?\n",
    "\n",
    "2. Generate three random sets of size 1000 using each of the three distributions. Calculate KL-Divergence between $P$ and $Q$, and between $P$ and $R$. Do we get similar results?\n",
    "\n",
    "Note:\n",
    "You can generate sets using `np.random.choice`. For example, for distribution $P$ is `[np.random.choice(np.arange(1, 5), p=[0.5, 0.3, 0.1, 0.1]) for i in range(1000)]`\n",
    "\n",
    "You can compute KL-divergence between sets $P$ and $Q$ using `entropy(P,Q)` from `from scipy.stats import entropy`. And if you use `entropy(P)`, you get entropy of $P$. Both use natural log. If you wish to use base two log, add `base=2` inside entropy command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689fcdac-6558-49cd-b8a3-eb71f1b070a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
