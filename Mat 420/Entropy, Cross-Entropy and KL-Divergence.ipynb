{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c489d89c-c7c0-4926-bc53-f27e995d8cec",
      "metadata": {
        "id": "c489d89c-c7c0-4926-bc53-f27e995d8cec"
      },
      "source": [
        "## Information and Entropy\n",
        "\n",
        "Entropy is a concept from information theory designed to describe amount of uncertainty we have with a random variable in the given probability distribution. The idea is the more uncertain we are, the less information we have. To find entropy in essence we need to calculate average amount of information we need to describe a random variable. So, how would we measure information? We use \"bits\". To see how this works, let's look at two examples.\n",
        "\n",
        "#### Example 1\n",
        "Supppose we have the following distribution:\n",
        "\n",
        "$$Pr(x) = \\begin{cases}\n",
        "          0.25 & x =A \\\\\n",
        "          0.25 & x =B \\\\\n",
        "          0.25 & x =C \\\\\n",
        "          0.25 & x =D \\\\\n",
        "       \\end{cases}\n",
        "$$\n",
        "\n",
        "Given a random $x$, how many questions can we ask to determine what it equals to?\n",
        "Since all probabilties are the same, the mos efficient way of doing it is to ask two questions:\n",
        "\n",
        "1. Is it A or B?\n",
        "\n",
        "2. If the answer was \"Yes\", then we ask: Is it A?; If the answer was \"No\", we ask: Is it C?\n",
        "\n",
        "So, we always ask two questions, therefore the average number of questions asked is two as well. And so, we say that the entropy here is 2 bits.\n",
        "\n",
        "#### Example 2\n",
        "\n",
        "Suppose now our probabilities are not the same:\n",
        "\n",
        "$$Pr(x) = \\begin{cases}\n",
        "          0.5 & x =A \\\\\n",
        "          0.25 & x =B \\\\\n",
        "          0.125 & x =C \\\\\n",
        "          0.125 & x =D \\\\\n",
        "       \\end{cases}\n",
        "$$\n",
        "\n",
        "We can do the same as in the example 1, but can we do better? Well, Since the probability $Pr(x=A)$ is already a half, $A$ will appear very often, so let's ask our questions in the following way:\n",
        "\n",
        "1. Is it A?\n",
        "\n",
        "2. If the answer was \"No\", we ask: Is it B?\n",
        "\n",
        "3. If the answer was \"No\", we ask: Is it C?\n",
        "\n",
        "On the first glance, it may seem like we are asking more questions now, but it depends. If $x=A$ for example, we are asking only 1 question. So what is the average number of questions we are asking? To find this, we just need to calculate the Expected Value of $x$. Let $N(x)$ detone number of questions we are asking to get to $x$ (so $N(A)=1$ and $N(C)=3$ for example)\n",
        "\n",
        "$$E(x)=\\sum_{x=A, B, C, D } Pr(x) \\cdot N(x)=0.5 \\cdot 1+0.25\\cdot 2+0.125\\cdot 3+0.125\\cdot 3=1.75$$\n",
        "\n",
        "So, on average we are asking less than two questions and we say that the entropy for this distribution is 1.75 bits.\n",
        "\n",
        "In the example 2, we have less entropy, because we have more predictive power than inthe example 1.\n",
        "\n",
        "Let's formalize the our calculation. To find $N(x)$ notice that it depends on $Pr(x)$. The lower the probability, the questions we need to ask and since each question has binary answer, we need to look at powers of 2:\n",
        "\n",
        "$$N(x)=\\log_2\\left(\\frac{1}{Pr(x)}\\right)$$\n",
        "\n",
        "So, suppose we have some probability distribution $P$ in which $P(i)=p_i$, then the entropy is:\n",
        "\n",
        "$$H(P)=\\sum_{i}p_i\\cdot\\log_2(1/p_i) = - \\sum_{i}p_i\\cdot\\log_2(p_i)$$\n",
        "\n",
        "Finally, we will only care about entropy as a relative quantity and so we can replace $\\log_2()$ with $\\ln()$ as they differ only by a constant:\n",
        "\n",
        "$$H(P)= - \\sum_{i}p_i\\cdot\\ln(p_i)$$\n",
        "\n",
        "### Example 3\n",
        "\n",
        "Suppose I have a bag with 10 blue and 10 red marbles and I want to pick one at random, then I can't really predict what kind of marble I will get, but if the bag had 19 red marbles and 1 blue marble, then I should be expecting to get a red marble. So in the first case I should have have larger entropy.\n",
        "\n",
        "In our first bag case, entropy is\n",
        "\n",
        "$$H= -\\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\approx 0.6931$$\n",
        "\n",
        "And in the second case:\n",
        "\n",
        "$$H=-\\frac{1}{20}\\ln\\left(\\frac{1}{20}\\right)-\\frac{19}{20}\\ln\\left(\\frac{19}{20}\\right) \\approx 0.198514$$\n",
        "\n",
        "\n",
        "Note: If we $p_k=0$, we would get $0 \\cdot \\ln(0)$ which is undefined. However, since 0 probability is perfect certainty, we set this to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cross-Entropy and KL-Divergence\n",
        "\n",
        "\n",
        "Suppose we have two distributions from examples 1 and 2. I will think of them as some 4-sided dice. I will call first example distribution as $Q$ and the other as $P$. I would like to compare these two distributions. To do that we will look at the probabilities of getting specific samples. In other words, suppose I roll die $P$ 16 times, then we would expect to get 8 $A$'s, 4 $B$'s , 2 $C$'s and 2 $D$'s. (In fact, if we roll the die $N$ times then the expected amount of let's say $A$'s is $N \\cdot Pr(A) = N \\cdot 0.5$.)\n",
        "\n",
        "Suppose we get these values in this specific order. Then the probability of getting this sequence is\n",
        "\n",
        "$$Prob=0.5^8 \\cdot 0.25^4 \\cdot 0.125^2 \\cdot 0.125^2$$\n",
        "\n",
        "This value is very tiny and as $N$ grows, it get even smaller. To fix this we will take $N$'s root to normalize it with respect to sample size. Since the powers in the above calculations are of the form $N\\cdot Pr$, this root basically will eliminate $N$'s from the power, leaving just the probability.\n",
        "\n",
        "The second problem is that our numbers are still quite small and we are multiplying them which makes it even worse. Usual way to fix it is to take logarithm. We will take negative logarithm to keep our answer positive:\n",
        "\n",
        "$$-\\ln\\left(Prob^{1/N}\\right)=-\\frac{8}{16}\\ln(0.5)-\\frac{4}{16}\\ln(0.25)-\\frac{2}{16}\\ln(0.125)-\\frac{2}{16}\\ln(0.125)$$\n",
        "\n",
        "$$=-0.5\\ln(0.5)-0.25\\ln(0.25)-0.125\\ln(0.125)-0.125\\ln(0.125) \\approx 1.213$$\n",
        "\n",
        "Notice what we got is just entropy of $P$: $H(P)=-\\sum_i p_i \\ln(p_i)$. However, in this case I will think of as cross-entropy of $P$ with respect to itself (I am using specific distribution to get expected distribution in my sample and I am using specific distribution to calculate the probability of getting that sample. It just happens to be the same distribution.)\n",
        "\n",
        "So, what if we look for probability of getting the same sample but using die $Q$? The closer the $Q$ is to $P$ the closer the probabilites should be to the ones we got with a die $P$. So, we calculate:\n",
        "\n",
        "$$Prob=0.25^8 \\cdot 0.25^4 \\cdot 0.25^2 \\cdot 0.25^2$$\n",
        "\n",
        "Performing the same normalization:\n",
        "\n",
        "$$-\\ln\\left(Prob^{1/N}\\right)=-0.5\\ln(0.25)-0.25\\ln(0.25)-0.125\\ln(0.25)-0.125\\ln(0.25) \\approx 1.386$$\n",
        "\n",
        "What we got is a cross-entropy of $P$ relative to $Q$. In general this is if we use $p_i$'s as probabilties from $P$ and $q_i$'s as probabilities from $Q$, the cross-entropy is equal to\n",
        "\n",
        "$$H(P,Q)=-\\sum_i p_i \\ln(q_i)$$\n",
        "\n",
        "Few properties of cross-entropy:\n",
        "\n",
        "1. $H(P,P)=H(P)$\n",
        "\n",
        "2. $H(P,Q) \\geq H(P) \\geq 0$\n",
        "\n",
        "3. In general, $H(P,Q) \\neq H(Q,P)$\n",
        "\n",
        "\n",
        "Using these cross-entropies we can measure how far $Q$ is from $P$ by subtraction:\n",
        "\n",
        "$$D_{KL}(P||Q)= H(P,Q)-H(P)$$\n",
        "\n",
        "This is called KL-Divergence. So, in our example,\n",
        "\n",
        "$$D_{KL}(P||Q) \\approx 1.386- 1.213=0.173$$\n",
        "\n",
        "Properties of KL-Divergence:\n",
        "\n",
        "1. $D_{KL}(P||Q) \\geq 0$\n",
        "\n",
        "2. $D_{KL}(P||P) = 0$\n",
        "\n",
        "3. In general, $D_{KL}(P||Q) \\neq D_{KL}(Q||P) $\n",
        "\n",
        "### Example 4\n",
        "\n",
        "Suppose we have a third die $R$ with the following likelihoods:\n",
        "\n",
        "$$Pr(x) = \\begin{cases}\n",
        "          0.1 & x =A \\\\\n",
        "          0.1 & x =B \\\\\n",
        "          0.4 & x =C \\\\\n",
        "          0.4 & x =D \\\\\n",
        "       \\end{cases}\n",
        "$$\n",
        "\n",
        "If you look carefully, it should be quite obvious that $R$ is much further from $P$ than $Q$ was. So let's see that by calculating $D_{KL}(P||R)$\n",
        "\n",
        "$$H(P, R)=-0.5\\ln(0.1)-0.25\\ln(0.1)-0.125\\ln(0.4)-0.125\\ln(0.4) \\approx 1.956$$\n",
        "\n",
        "And so, $$D_{KL}(P||R) \\approx 1.956- 1.213=0.743$$"
      ],
      "metadata": {
        "id": "1fBXQnsJ2I2E"
      },
      "id": "1fBXQnsJ2I2E"
    },
    {
      "cell_type": "markdown",
      "id": "c11b3b12",
      "metadata": {
        "id": "c11b3b12"
      },
      "source": [
        "### Cross-Entropy Loss\n",
        "\n",
        "Suppose we have some labeled training data and we have some classification model parametrized with some parameter $\\theta$. Then, given a data point $x$, our model produces a prediction $\\hat{y}$. This prediction is a vector with number of entries equal to number of classes. Each entry is the probability that our data point belongs to a corresponding class. For example, if\n",
        "$$\\hat{y}=\\begin{bmatrix} 0.2 \\\\ 0.3 \\\\0.5 \\end{bmatrix}$$\n",
        "Then the probability that $x$ belongs to class 0 is 0.2, the probability that $x$ belongs to class 1 is 0.3, and the probability that $x$ belongs to class 2 is 0.5. So, we get a distribution for $x$. I will denote this as $$ \\hat{P}(x|\\theta)=\\begin{bmatrix} \\hat{p}_1 \\\\ \\hat{p}_2 \\\\\\hat{p}_3 \\end{bmatrix}$$\n",
        "\n",
        "However, our data is labeled, so we do have the actual label $y$ for $x$. This is also a probability vector. However, it contains a single 1 in the position corresponding to a true class of $x$, and it has zeros everywhere else. Nevertheless, it is still a distribution, and I will denote it as\n",
        "\n",
        "$$P(x)=\\begin{bmatrix} {p}_1 \\\\ {p}_2 \\\\{p}_3 \\end{bmatrix}$$ Note, that it doesn't depend on $\\theta$ since it doesn't come from our training model.\n",
        "\n",
        "We would like our model to predict the correct class, so we want $ \\hat{P}(x|\\theta)$ to be close to $P(x)$. We can use KL-divergence to see how close two distributions are and try to minimize it.\n",
        "\n",
        "$$D_{KL}\\left(P(x) || \\hat{P}(x|\\theta)\\right)=H\\left(P(x), \\hat{P}(x|\\theta)\\right) - H\\left(P(x)\\right)$$\n",
        "\n",
        "Two things to note here:\n",
        "\n",
        "1. We can not reverse distributions inside $D_{KL}$, since then we would have to use entropy of $\\hat{P}$. However, we don't want this entropy to affect our optimization.\n",
        "\n",
        "2. Since $H(P(x))$ doesn't depend on $\\theta$, our optimization doesn't depend on this term. In other words, thr parameter $\\theta$ that minimizes KL-divergence will also minimize cross-entropy:\n",
        "\n",
        "$$\\mathop{\\arg\\min}_{\\theta} D_{KL}\\left(P(x) || \\hat{P}(x|\\theta)\\right) = \\mathop{\\arg\\min}_{\\theta}H\\left(P(x), \\hat{P}(x|\\theta)\\right)$$\n",
        "\n",
        "So, we now define the Cross-Entropy Loss function for a single datapoint $x$ as follows :\n",
        "\n",
        "$$CELoss(x)=H\\left(P(x), \\hat{P}(x|\\theta)\\right)=-\\sum_i p_i\\ln(\\hat{p}_i)$$\n",
        "\n",
        "And for the whole data set, as before, we add losses for each point in the data set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relation of CELoss to NLLoss\n",
        "\n",
        "When we have only two classes, then\n",
        "\n",
        "$$P(x)=\\begin{bmatrix} p \\\\ 1-p  \\end{bmatrix}, \\hat{P}(x)=\\begin{bmatrix} \\hat{p} \\\\ 1-\\hat{p}  \\end{bmatrix}$$\n",
        "\n",
        "Then,\n",
        "\n",
        "$$CELoss(x)=-p\\ln(\\hat{p})-(1-p)\\ln(1-\\hat{p})=NLLoss(x)$$"
      ],
      "metadata": {
        "id": "RshaMBH2HOqx"
      },
      "id": "RshaMBH2HOqx"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7VOes0rKkXI"
      },
      "id": "d7VOes0rKkXI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}