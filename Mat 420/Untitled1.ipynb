{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1f7e2d",
   "metadata": {},
   "source": [
    "## KL-Divergence\n",
    "\n",
    "KL-Divergence (also known as Relative Entropy) is a way to compare two probability distributions. This comparison is one directional, meaning one of the distribution is taken as ground truth and the other as the testing distribution. This means KL-Divergence is not symmetric. Let's try to develop this measure. We will consider $P$ as true distribution ( or observations) and $Q$ as testing distribution (theory or predictions).\n",
    "\n",
    "Suppose we have a coin. As we flip our coin many times we record its distribution. Suppose we get $Pr(T|Q)=0.25$ and $Pr(H|Q)=0.75$. We want to compare this distribution to the distribution of a fair coin: $Pr(T|P)=Pr(H|P)=0.5$. One way we can do this is by looking at large random samples, computing likelihoods of such sample according to each distribution and finding a ratio of the two. \n",
    "\n",
    "For example, let's say we have $S=\\{H, T, T, H, H, H, H, T, T, H \\}$. Let $N_H$ and $N_T$ be number of heads and tails, so $N_H=6$, $N_T=4$. Then\n",
    "\n",
    "$$Pr(S|P)=Pr(H|P)^{N_H}\\cdot Pr(H|P)^{N_H}=\\left(\\frac{1}{2}\\right)^6 \\cdot \\left(\\frac{1}{2}\\right)^4=\\left(\\frac{1}{2}\\right)^{10} \\approx 0.00097656$$\n",
    "$$Pr(S|Q)=Pr(H|Q)^{N_H}\\cdot Pr(H|Q)^{N_H}=\\left(\\frac{3}{4}\\right)^6 \\cdot \\left(\\frac{1}{4}\\right)^4=\\frac{3^6}{4^{10}}\\approx 0.00069523$$\n",
    "\n",
    "Taking the ratio:\n",
    "\n",
    "$$\\frac{Pr(S|P)}{Pr(S|Q)}=\\frac{0.00097656}{0.00069523} \\approx 1.404664$$\n",
    "\n",
    "This is almost what we want. There are three things we need to fix:\n",
    "\n",
    "1. So far this depends on sample. If we want to compare distributions we would want this to be independed of a sample taken. We can achieve this by leting our sample size go to infinity.\n",
    "\n",
    "2. Above creates a new problem. If sample size is infinite, then the powers we take will create problems. So, we normalize this taking $1/N$ power of our results, where $N$ is a sample size. Note that in this case $N_H/N$ and $N_T/N$ will approach the corresponding probabilities of the true distribution.\n",
    "\n",
    "3. Finally, we want the value to be similar if for example probability is doubled or halved. We will take natural log to achive this, since $\\ln(2)= -\\ln(1/2)$\n",
    "\n",
    "\n",
    "Let's create and finalize our formula and apply to our example.\n",
    "\n",
    "Let $Pr(T|P)=p_1$ and $Pr(H|P)=p_2$; let $Pr(T|Q)=q_1$ and $Pr(H|Q)=q_2$. Then the ratio we had before would be:\n",
    "\n",
    "$$\\frac{p_1^{N_T}p_2^{N_H}}{q_1^{N_T}q_2^{N_H}}$$.\n",
    "\n",
    "Let's take the power $1/N$ and a natural log:\n",
    "\n",
    "$$\\ln\\left(\\left(\\frac{p_1^{N_T}p_2^{N_H}}{q_1^{N_T}q_2^{N_H}}\\right)^{1/N}\\right)=\\frac{1}{N}\\left(\\ln(p_1^{N_T})+\\ln(p_2^{N_H})-\\ln(q_1^{N_T})-\\ln(q_2^{N_H})\\right)$$\n",
    "$$= \\frac{N_T}{N}\\ln(p_1)+\\frac{N_H}{N}\\ln(p_2)-\\frac{N_T}{N}\\ln(q_1)-\\frac{N_H}{N}\\ln(q_2)$$\n",
    "\n",
    "And as $N \\rightarrow \\infty$, we get:\n",
    "\n",
    "$$ p_1\\ln(p_1)+p_2\\ln(p_2)-p_1\\ln(q_1)-p_2\\ln(q_2) = p_1 \\ln \\left(\\frac{p_1}{q_1}\\right) + p_2\\ln\\left(\\frac{p_2}{q_2}\\right)=\\sum_i p_i\\ln \\left(\\frac{p_i}{q_i}\\right)$$\n",
    "\n",
    "This is the KL-Divergence. The notation for it is\n",
    "\n",
    "$$D_{KL}(P||Q)=\\sum_i p_i\\ln \\left(\\frac{p_i}{q_i}\\right),$$\n",
    "where $p_i=Pr(i|P)$ and $q_i=Pr(i|Q)$\n",
    "\n",
    "Applying to our coin example, we would get:\n",
    "\n",
    "$$D_{KL}(P||Q)=\\frac{1}{2}\\ln\\left(\\frac{1/2}{3/4}\\right)+\\frac{1}{2}\\ln\\left(\\frac{1/2}{1/4}\\right)\\approx 0.14384 $$\n",
    "\n",
    "If, for example, our $Q$ distribution was $Pr(T|Q)=0.1$ and $Pr(H|Q)=0.9$, then we should expect to get larger value for KL-Divergence:\n",
    "\n",
    "$$D_{KL}(P||Q)=\\frac{1}{2}\\ln\\left(\\frac{1/2}{9/10}\\right)+\\frac{1}{2}\\ln\\left(\\frac{1/2}{1/10}\\right)\\approx 0.5108 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1209ad2",
   "metadata": {},
   "source": [
    "## Entropy and cross-Entropy\n",
    "\n",
    "There are many ways to define entropy. Originally this concept emerged from information theory to measure information gains. This is related to amount of disorder in our samples. For example, suppose I have a bag with 10 blue and 10 red marbles and I want to pick one at random, then I can't really predict what kind of marble I will get, but if the bag had 19 red marbles and 1 blue marble, then I should be expecting to get a red marble. We can describe this by saying that in the first case I have large entropy.\n",
    "\n",
    "Another way to see this is the average amount of surprise we expirience of each possible outcome. For example, in the first case, whether we pull red or blue marble should not make us surprised. In the second case, if we pull red marble, we shouldn't be surprised, but if we pull blue marble, this should be very surprising and on average we should have more surprise in the second example.\n",
    "\n",
    "To measure Entropy we use the following formula:\n",
    "\n",
    "$$H(P)=-\\sum_i p_i\\ln(p_i)$$\n",
    "\n",
    "For example, In our first bag case, entropy is \n",
    "\n",
    "$$H= -\\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\approx 0.6931$$\n",
    "\n",
    "And in the second case:\n",
    "\n",
    "$$H=-\\frac{1}{20}\\ln\\left(\\frac{1}{20}\\right)-\\frac{19}{20}\\ln\\left(\\frac{19}{20}\\right) \\approx 0.198514$$\n",
    "\n",
    "\n",
    "As we can see, entropy in the second case is much lower as expected.\n",
    "\n",
    "\n",
    "If we look carefully at KL-Divergence formula, we would notice that it is related to entropy:\n",
    "\n",
    "$$D_{KL}(P||Q)=\\sum_i p_i\\ln \\left(\\frac{p_i}{q_i}\\right)=\\sum_i p_i\\ln \\left(p_i\\right)-\\sum_i p_i\\ln \\left(q_i\\right)=-H(P)-\\sum_i p_i\\ln \\left(q_i\\right)$$\n",
    "\n",
    "The remaining sum in above formula is called cross entropy:\n",
    "\n",
    "$$H(P,Q)=-\\sum_i p_i\\ln \\left(q_i\\right)$$ and so final relation is:\n",
    "\n",
    "$$D_{KL}(P||Q)=H(P,Q)-H(P)$$\n",
    "\n",
    "So what is cross entropy? How is it different from KL-divergence?\n",
    "\n",
    "Both in essence are measures of how different two distributions are. If KL-Divergence is an extra suprise of distribution Q if P is actual distribution, then cross entropy is the total surprise of Q distribution if P is the true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b3b12",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss\n",
    "\n",
    "Suppose we have some labeled training data and we have some classification model paramterized with some parameter $\\theta$. Then given a data point $x^{(i)}$, our model produces a prediction $\\hat{y}^{(i)}$. This prediction is a vector with number of entries equal to number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6959702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
